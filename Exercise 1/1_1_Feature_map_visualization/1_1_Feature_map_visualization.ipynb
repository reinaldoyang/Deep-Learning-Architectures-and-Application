{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XhRIufI0Xhn1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbUR-5jd3x0o"
      },
      "source": [
        "##**Example1~5**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_hYzFka9Xj87"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(cv2im, resize_im=True):\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        cv2im = cv2.resize(cv2im, (224, 224))\n",
        "    im_as_arr = np.float32(cv2im)\n",
        "    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::-1])\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "59727d50ce744030956e17429411edba",
            "8265db1cf159482f9718b78fe2a833e9",
            "1e6f906da1784f8990586f2ce6974a70",
            "568ac00b2e944f249589a74212775845",
            "55de07f9573b4c55ac4dc719276b5b9b",
            "c670505f2f68407ea432508c257161c1",
            "79bfb01f6c874aae861b54bfc6300d28",
            "7b50335e2b5949c5a054726675b82ce0",
            "b202d5386a6346a2adb9d40d83528148",
            "0be0831dfa2c4161a344cca16231dc89",
            "ec564ffa02114d539314cdc1da407fa7"
          ]
        },
        "id": "LkptXxluXmbL",
        "outputId": "27d4a23d-a972-48bc-fa21-8a3910397951"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Reinaldo yang/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:27<00:00, 20.4MB/s] \n"
          ]
        }
      ],
      "source": [
        "class FeatureVisualization():\n",
        "    def __init__(self,img_path,selected_layer,models=models.vgg16(pretrained=True)):\n",
        "        self.img_path=img_path\n",
        "        self.selected_layer=selected_layer\n",
        "        # if models==None:\n",
        "        #   models = models.vgg16(pretrained=True)\n",
        "        # else:\n",
        "        #   models = models\n",
        "        # Load pretrained model\n",
        "        \n",
        "        self.pretrained_model = models.features.cuda()\n",
        "        self.pretrained_model.eval()\n",
        "        self.pretrained_model2 = models.cuda()\n",
        "        #self.pretrained_model2 = models.resnet18(pretrained=True)\n",
        "        self.pretrained_model2.eval()\n",
        "    def process_image(self):\n",
        "        img=cv2.imread(self.img_path)\n",
        "        img=preprocess_image(img)\n",
        "        return img\n",
        "\n",
        "    def get_feature(self):\n",
        "        # Image  preprocessing\n",
        "        input=self.process_image()\n",
        "        #print(\"input.shape:{}\".format(input.shape))\n",
        "        x=input.cuda()\n",
        "        for index,layer in enumerate(self.pretrained_model):\n",
        "            x=layer(x)\n",
        "            #print(\"x:{}\".format(x.shape))\n",
        "            if (index == self.selected_layer):\n",
        "                return x\n",
        "\n",
        "    def get_single_feature(self):\n",
        "        # Get the feature map\n",
        "\n",
        "        features=self.get_feature()\n",
        "        print(features.shape)\n",
        "        feature=features[:,0,:,:]\n",
        "        feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "\n",
        "        #print(\"feature\")\n",
        "        #print(feature.shape)\n",
        "        return feature\n",
        "\n",
        "    def get_multi_feature(self):\n",
        "        # Get the feature map\n",
        "        features=self.get_feature()\n",
        "        #print(features.shape)\n",
        "        result_path = './feat_first' + str(self.selected_layer)\n",
        "\n",
        "        if not os.path.exists(result_path):\n",
        "            os.makedirs(result_path)\n",
        "        print(\"On layer:{}, We can get the {} feature maps\".format(self.selected_layer,features.shape[1]))    \n",
        "        #print(features.shape[1])\n",
        "        for i in range(features.shape[1]):\n",
        "            feature=features[:,i,:,:]\n",
        "            feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "            feature = feature.data.cpu().numpy()\n",
        "            feature = 1.0 / (1 + np.exp(-1 * feature))\n",
        "            feature = np.round(feature * 255)\n",
        "            save_name = result_path + '/' + str(i) + '.jpg'\n",
        "            cv2.imwrite(save_name, feature)\n",
        "    def get_multi_feature1(self):\n",
        "        # Get the feature map\n",
        "        features=self.get_feature()\n",
        "        #print(features.shape)\n",
        "        result_path = './feat_second' + str(self.selected_layer)\n",
        "\n",
        "        if not os.path.exists(result_path):\n",
        "            os.makedirs(result_path)\n",
        "        print(\"On layer:{}, We can get the {} feature maps\".format(self.selected_layer,features.shape[1]))    \n",
        "        #print(features.shape[1])\n",
        "        for i in range(features.shape[1]):\n",
        "            feature=features[:,i,:,:]\n",
        "            feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "            feature = feature.data.cpu().numpy()\n",
        "            feature = 1.0 / (1 + np.exp(-1 * feature))\n",
        "            feature = np.round(feature * 255)\n",
        "            save_name = result_path + '/' + str(i) + '.jpg'\n",
        "            cv2.imwrite(save_name, feature)\n",
        "\n",
        "    def save_feature_to_img(self,epoch=None):\n",
        "        #to numpy\n",
        "        feature=self.get_single_feature()\n",
        "        self.get_multi_feature()\n",
        "        feature=feature.data.cpu().numpy()\n",
        "\n",
        "        #use sigmod to [0,1]\n",
        "        # print(feature[0])\n",
        "        feature= 1.0/(1+np.exp(-1*feature))\n",
        "\n",
        "        # to [0,255]\n",
        "        feature=np.round(feature*255)\n",
        "        #print(self.selected_layer)\n",
        "        if epoch==None:\n",
        "          save_name = './feat_first' + str(self.selected_layer) + '.jpg'\n",
        "          cv2.imwrite(save_name, feature)\n",
        "        else:\n",
        "          save_path = './feature_per_epoch/'\n",
        "          if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "          save_name = save_path+ 'feat_first' + str(self.selected_layer) +'_'+str(epoch)+ '.jpg'\n",
        "          \n",
        "          cv2.imwrite(save_name, feature)\n",
        "    def save_feature_to_img1(self):\n",
        "        #to numpy\n",
        "        feature=self.get_single_feature()\n",
        "        self.get_multi_feature1()\n",
        "        feature=feature.data.cpu().numpy()\n",
        "\n",
        "        #use sigmod to [0,1]\n",
        "        # print(feature[0])\n",
        "        feature= 1.0/(1+np.exp(-1*feature))\n",
        "\n",
        "        # to [0,255]\n",
        "        feature=np.round(feature*255)\n",
        "        #print(self.selected_layer)\n",
        "        save_name = './feat_second' + str(self.selected_layer) + '.jpg'\n",
        "        cv2.imwrite(save_name, feature)\n",
        "    def plot_probablity(self,outputs):\n",
        "\n",
        "        outputs = outputs.data.cpu().numpy()\n",
        "        outputs = np.ndarray.tolist(outputs)\n",
        "        print(outputs[0])\n",
        "        x = range(0, 1000)\n",
        "        plt.bar(x, outputs[0])\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Probablity\")\n",
        "        plt.title(\"Image classifier\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        input=self.process_image().cuda()\n",
        "        outputs = self.pretrained_model2(input)\n",
        "        \n",
        "        s = torch.nn.Softmax(dim=1)\n",
        "        result = s(outputs)\n",
        "        # self.plot_probablity(result)\n",
        "        \n",
        "        prob, predicted = result.sort(1,descending=True)\n",
        "        prob = prob.data.cpu().numpy()\n",
        "\n",
        "        predicted = predicted.data.cpu().numpy()\n",
        "        \n",
        "        print(\"Probablity TOP-3:\\n\")\n",
        "        print(\"\")\n",
        "        for i in range(3):\n",
        "            \n",
        "            print(\"TOP_\"+str(i+1))\n",
        "            print(\"Probablity:{}\".format(prob[0][i]))\n",
        "            print(\"Predicted:{}\\n\".format(c[int(predicted[0][i])]))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLeGlPjYwrkX"
      },
      "source": [
        "##**Visualize the feature maps extracted from the layer-index-5 and layer-index-10**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "KGgajl_4XogN",
        "outputId": "abfeede3-0f5b-4235-f851-f773400a51d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([1, 128, 112, 112])\n",
            "On layer:5, We can get the 128 feature maps\n",
            "Probablity TOP-3:\n",
            "\n",
            "\n",
            "TOP_1\n",
            "Probablity:0.5287237167358398\n",
            "Predicted: 'Pembroke\n",
            "\n",
            "TOP_2\n",
            "Probablity:0.40296363830566406\n",
            "Predicted: 'Cardigan\n",
            "\n",
            "TOP_3\n",
            "Probablity:0.06319644302129745\n",
            "Predicted: 'basenji'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "  # get class\n",
        "  c = {}\n",
        "  with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
        "    for line in f:\n",
        "      (key, val) = line.split(\":\")\n",
        "      c[int(key)] = val.split(\",\")[0]\n",
        "  # Define image path and select the layer\n",
        "  myClass=FeatureVisualization('./dog.jpg',5)\n",
        "  print(myClass.pretrained_model2)\n",
        "\n",
        "  myClass.save_feature_to_img()\n",
        "  myClass.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVzG3n92xwxA"
      },
      "source": [
        "##**Example 6~8**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RERQbbC31hpn",
        "outputId": "f3da8180-8a71-4a2c-a5ac-e597e22f0a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([1, 128, 112, 112])\n",
            "On layer:5, We can get the 128 feature maps\n",
            "torch.Size([1, 128, 112, 112])\n",
            "On layer:5, We can get the 128 feature maps\n",
            "The first picture classification predict:\n",
            "Probablity TOP-3:\n",
            "\n",
            "\n",
            "TOP_1\n",
            "Probablity:0.5287237167358398\n",
            "Predicted: 'Pembroke\n",
            "\n",
            "TOP_2\n",
            "Probablity:0.40296363830566406\n",
            "Predicted: 'Cardigan\n",
            "\n",
            "TOP_3\n",
            "Probablity:0.06319644302129745\n",
            "Predicted: 'basenji'\n",
            "\n",
            "The second picture classification predict:\n",
            "Probablity TOP-3:\n",
            "\n",
            "\n",
            "TOP_1\n",
            "Probablity:0.5287237167358398\n",
            "Predicted: 'Pembroke\n",
            "\n",
            "TOP_2\n",
            "Probablity:0.40296363830566406\n",
            "Predicted: 'Cardigan\n",
            "\n",
            "TOP_3\n",
            "Probablity:0.06319644302129745\n",
            "Predicted: 'basenji'\n",
            "\n",
            "Verification:\n",
            "They are not the same!\n",
            "Their cosine_distance:tensor([1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "Their euclidean_dist:0.0\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    # get class\n",
        "    c = {}\n",
        "    with open(\"imagenet1000_clsidx_to_labels.txt\") as f:\n",
        "        for line in f:\n",
        "            (key, val) = line.split(\":\")\n",
        "            c[int(key)] = val.split(\",\")[0]\n",
        "    # Define image path and select the layer\n",
        "    myClass=FeatureVisualization('./dog.jpg',5)\n",
        "    Compare=FeatureVisualization('./dog.jpg',5)    \n",
        "    print(myClass.pretrained_model2)\n",
        "\n",
        "    myClass.save_feature_to_img()\n",
        "    Compare.save_feature_to_img1()\n",
        "    print(\"The first picture classification predict:\")\n",
        "    myClass_vector = myClass.predict()\n",
        "    print(\"The second picture classification predict:\")\n",
        "    Compare_vector = Compare.predict()\n",
        "    #Define cosine similarity\n",
        "    cos= nn.CosineSimilarity(dim=1)\n",
        "    #Define Euclidean distance\n",
        "    euclidean_dist = torch.dist(myClass_vector,Compare_vector,p=2)\n",
        "    cosine_dist = cos(myClass_vector,Compare_vector)\n",
        "    print(\"Verification:\")\n",
        "    if cosine_dist < 0.6:\n",
        "        print(\"They are the same!\")\n",
        "        print(\"Their cosine_distance:{}\".format(cosine_dist))\n",
        "    else:\n",
        "        print(\"They are not the same!\")\n",
        "        print(\"Their cosine_distance:{}\".format(cosine_dist))\n",
        "       \n",
        "    print(\"Their euclidean_dist:{}\".format(euclidean_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8_Fy6zX4auS"
      },
      "source": [
        "##**Example 9~13**##\n",
        "##**Use the VGG-16 model pretrained on ImageNet to train the CIFAR-100 dataset**##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWjr9B2j1LGQ",
        "outputId": "36f6bbfa-f739-4c64-a7d4-589584201104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 28999680/169001437 [04:22<21:05, 110661.15it/s] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m input_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m     10\u001b[0m num_epoch \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mCIFAR100(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m'\u001b[39;49m, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[0;32m     13\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(trainset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m testset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR100(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m train  \u001b[39m# training set or test set\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[0;32m     68\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFiles already downloaded and verified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m download_and_extract_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, md5\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtgz_md5)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:447\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename:\n\u001b[0;32m    445\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[1;32m--> 447\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[0;32m    449\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    450\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:157\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fpath)\n\u001b[1;32m--> 157\u001b[0m     _urlretrieve(url, fpath)\n\u001b[0;32m    158\u001b[0m \u001b[39mexcept\u001b[39;00m (urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m url[:\u001b[39m5\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m), filename, length\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mlength)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:37\u001b[0m, in \u001b[0;36m_save_response_content\u001b[1;34m(content, destination, length)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_response_content\u001b[39m(\n\u001b[0;32m     32\u001b[0m     content: Iterator[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m     33\u001b[0m     destination: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     34\u001b[0m     length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(destination, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fh, tqdm(total\u001b[39m=\u001b[39mlength) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m---> 37\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m content:\n\u001b[0;32m     38\u001b[0m             \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[0;32m     40\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39m(\u001b[39mlambda\u001b[39;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m), filename, length\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mlength)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:462\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 462\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:506\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    501\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    503\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    508\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mc:\\Users\\Reinaldo yang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "                 transforms.Resize(32), \n",
        "                 transforms.ToTensor(), \n",
        "                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "input_size = 32\n",
        "num_epoch = 1\n",
        " \n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c620a4743ab746eaba119c5e0f88baeb",
            "33cd3b4683824246bc0ff65dbc28fb44",
            "de1f61a36dd94a89a0310b757388a508",
            "b2097300b0604e5a8947a82dc5a53c47",
            "1d6cab8050fb48be8393eaecf1ab9a29",
            "914641222f7a4db5864a9d93aab1a5bf",
            "2d98e47f9f994dd58803803e22aff44f",
            "7305c53fb858407a8de9c3949e01d6f3",
            "eaa851f77b2f47b796d8cb25191e8945",
            "9dabfb206db74bd3a18fea243160e06f",
            "25015bf4c85e4678ba47d0e29efdee0f"
          ]
        },
        "id": "jyjD6b3Y4Zxt",
        "outputId": "1ef03e8a-bcea-40b6-bc3d-a1afe4314919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c620a4743ab746eaba119c5e0f88baeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=1024, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "my_model = torchvision.models.vgg16_bn(pretrained=True)\n",
        "# print(my_model)\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "\n",
        "# my_model.features = torch.nn.Sequential(\n",
        "#     nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "#     nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "#     nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "#     nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
        "#     nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "#     nn.ReLU(inplace=True),\n",
        "#     nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "# )\n",
        "# my_model.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "my_model.classifier = torch.nn.Sequential(torch.nn.Linear(25088, 4096),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Dropout(p=0.5),\n",
        "                                       torch.nn.Linear(4096, 1024),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Dropout(p=0.5),\n",
        "                                       torch.nn.Linear(1024, 100))\n",
        "print(my_model)\n",
        "\n",
        "my_model = my_model.cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "# optimizer = optim.SGD(my_model.classifier.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "# print(my_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8m65gpc4xw9",
        "outputId": "0d2a7e74-8f0c-4425-9f7c-007be8b5e3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 112, 112])\n",
            "On layer:7, We can get the 128 feature maps\n",
            "epochs:　1, steps: 1 loss: 4.604\n",
            "epochs:　1, steps: 2 loss: 4.904\n",
            "epochs:　1, steps: 3 loss: 4.537\n",
            "epochs:　1, steps: 4 loss: 4.638\n",
            "epochs:　1, steps: 5 loss: 4.641\n",
            "epochs:　1, steps: 6 loss: 4.646\n",
            "epochs:　1, steps: 7 loss: 4.574\n",
            "epochs:　1, steps: 8 loss: 4.557\n",
            "epochs:　1, steps: 9 loss: 4.592\n",
            "epochs:　1, steps: 10 loss: 4.540\n",
            "epochs:　1, steps: 11 loss: 4.498\n",
            "epochs:　1, steps: 12 loss: 4.559\n",
            "epochs:　1, steps: 13 loss: 4.399\n",
            "epochs:　1, steps: 14 loss: 4.539\n",
            "epochs:　1, steps: 15 loss: 4.603\n",
            "epochs:　1, steps: 16 loss: 4.555\n",
            "epochs:　1, steps: 17 loss: 4.411\n",
            "epochs:　1, steps: 18 loss: 4.523\n",
            "epochs:　1, steps: 19 loss: 4.474\n",
            "epochs:　1, steps: 20 loss: 4.490\n",
            "epochs:　1, steps: 21 loss: 4.493\n",
            "epochs:　1, steps: 22 loss: 4.510\n",
            "epochs:　1, steps: 23 loss: 4.554\n",
            "epochs:　1, steps: 24 loss: 4.488\n",
            "epochs:　1, steps: 25 loss: 4.523\n",
            "epochs:　1, steps: 26 loss: 4.480\n",
            "epochs:　1, steps: 27 loss: 4.592\n",
            "epochs:　1, steps: 28 loss: 4.457\n",
            "epochs:　1, steps: 29 loss: 4.499\n",
            "epochs:　1, steps: 30 loss: 4.466\n",
            "epochs:　1, steps: 31 loss: 4.417\n",
            "epochs:　1, steps: 32 loss: 4.482\n",
            "epochs:　1, steps: 33 loss: 4.489\n",
            "epochs:　1, steps: 34 loss: 4.487\n",
            "epochs:　1, steps: 35 loss: 4.415\n",
            "epochs:　1, steps: 36 loss: 4.500\n",
            "epochs:　1, steps: 37 loss: 4.516\n",
            "epochs:　1, steps: 38 loss: 4.445\n",
            "epochs:　1, steps: 39 loss: 4.496\n",
            "epochs:　1, steps: 40 loss: 4.502\n",
            "epochs:　1, steps: 41 loss: 4.475\n",
            "epochs:　1, steps: 42 loss: 4.444\n",
            "epochs:　1, steps: 43 loss: 4.375\n",
            "epochs:　1, steps: 44 loss: 4.468\n",
            "epochs:　1, steps: 45 loss: 4.449\n",
            "epochs:　1, steps: 46 loss: 4.380\n",
            "epochs:　1, steps: 47 loss: 4.330\n",
            "epochs:　1, steps: 48 loss: 4.416\n",
            "epochs:　1, steps: 49 loss: 4.442\n",
            "epochs:　1, steps: 50 loss: 4.355\n",
            "epochs:　1, steps: 51 loss: 4.601\n",
            "epochs:　1, steps: 52 loss: 4.424\n",
            "epochs:　1, steps: 53 loss: 4.424\n",
            "epochs:　1, steps: 54 loss: 4.388\n",
            "epochs:　1, steps: 55 loss: 4.428\n",
            "epochs:　1, steps: 56 loss: 4.461\n",
            "epochs:　1, steps: 57 loss: 4.330\n",
            "epochs:　1, steps: 58 loss: 4.383\n",
            "epochs:　1, steps: 59 loss: 4.413\n",
            "epochs:　1, steps: 60 loss: 4.456\n",
            "epochs:　1, steps: 61 loss: 4.370\n",
            "epochs:　1, steps: 62 loss: 4.371\n",
            "epochs:　1, steps: 63 loss: 4.389\n",
            "epochs:　1, steps: 64 loss: 4.435\n",
            "epochs:　1, steps: 65 loss: 4.341\n",
            "epochs:　1, steps: 66 loss: 4.302\n",
            "epochs:　1, steps: 67 loss: 4.356\n",
            "epochs:　1, steps: 68 loss: 4.404\n",
            "epochs:　1, steps: 69 loss: 4.300\n",
            "epochs:　1, steps: 70 loss: 4.247\n",
            "epochs:　1, steps: 71 loss: 4.208\n",
            "epochs:　1, steps: 72 loss: 4.248\n",
            "epochs:　1, steps: 73 loss: 4.351\n",
            "epochs:　1, steps: 74 loss: 4.272\n",
            "epochs:　1, steps: 75 loss: 4.190\n",
            "epochs:　1, steps: 76 loss: 4.284\n",
            "epochs:　1, steps: 77 loss: 4.260\n",
            "epochs:　1, steps: 78 loss: 4.387\n",
            "epochs:　1, steps: 79 loss: 4.136\n",
            "epochs:　1, steps: 80 loss: 4.187\n",
            "epochs:　1, steps: 81 loss: 4.236\n",
            "epochs:　1, steps: 82 loss: 4.135\n",
            "epochs:　1, steps: 83 loss: 4.132\n",
            "epochs:　1, steps: 84 loss: 4.264\n",
            "epochs:　1, steps: 85 loss: 4.223\n",
            "epochs:　1, steps: 86 loss: 4.109\n",
            "epochs:　1, steps: 87 loss: 4.120\n",
            "epochs:　1, steps: 88 loss: 4.241\n",
            "epochs:　1, steps: 89 loss: 4.082\n",
            "epochs:　1, steps: 90 loss: 4.109\n",
            "epochs:　1, steps: 91 loss: 4.255\n",
            "epochs:　1, steps: 92 loss: 4.128\n",
            "epochs:　1, steps: 93 loss: 4.093\n",
            "epochs:　1, steps: 94 loss: 4.199\n",
            "epochs:　1, steps: 95 loss: 4.101\n",
            "epochs:　1, steps: 96 loss: 4.048\n",
            "epochs:　1, steps: 97 loss: 4.065\n",
            "epochs:　1, steps: 98 loss: 4.032\n",
            "epochs:　1, steps: 99 loss: 4.128\n",
            "epochs:　1, steps: 100 loss: 3.911\n",
            "epochs:　1, steps: 101 loss: 4.130\n",
            "epochs:　1, steps: 102 loss: 4.060\n",
            "epochs:　1, steps: 103 loss: 3.941\n",
            "epochs:　1, steps: 104 loss: 3.963\n",
            "epochs:　1, steps: 105 loss: 3.981\n",
            "epochs:　1, steps: 106 loss: 3.990\n",
            "epochs:　1, steps: 107 loss: 4.024\n",
            "epochs:　1, steps: 108 loss: 3.881\n",
            "epochs:　1, steps: 109 loss: 3.926\n",
            "epochs:　1, steps: 110 loss: 3.973\n",
            "epochs:　1, steps: 111 loss: 4.088\n",
            "epochs:　1, steps: 112 loss: 4.024\n",
            "epochs:　1, steps: 113 loss: 3.993\n",
            "epochs:　1, steps: 114 loss: 3.939\n",
            "epochs:　1, steps: 115 loss: 3.989\n",
            "epochs:　1, steps: 116 loss: 3.824\n",
            "epochs:　1, steps: 117 loss: 3.867\n",
            "epochs:　1, steps: 118 loss: 3.892\n",
            "epochs:　1, steps: 119 loss: 3.823\n",
            "epochs:　1, steps: 120 loss: 3.840\n",
            "epochs:　1, steps: 121 loss: 3.819\n",
            "epochs:　1, steps: 122 loss: 3.820\n",
            "epochs:　1, steps: 123 loss: 3.837\n",
            "epochs:　1, steps: 124 loss: 3.884\n",
            "epochs:　1, steps: 125 loss: 3.892\n",
            "epochs:　1, steps: 126 loss: 3.855\n",
            "epochs:　1, steps: 127 loss: 3.757\n",
            "epochs:　1, steps: 128 loss: 3.776\n",
            "epochs:　1, steps: 129 loss: 3.807\n",
            "epochs:　1, steps: 130 loss: 3.917\n",
            "epochs:　1, steps: 131 loss: 3.856\n",
            "epochs:　1, steps: 132 loss: 3.833\n",
            "epochs:　1, steps: 133 loss: 3.658\n",
            "epochs:　1, steps: 134 loss: 3.673\n",
            "epochs:　1, steps: 135 loss: 3.893\n",
            "epochs:　1, steps: 136 loss: 3.826\n",
            "epochs:　1, steps: 137 loss: 3.718\n",
            "epochs:　1, steps: 138 loss: 3.704\n",
            "epochs:　1, steps: 139 loss: 3.743\n",
            "epochs:　1, steps: 140 loss: 3.775\n",
            "epochs:　1, steps: 141 loss: 3.721\n",
            "epochs:　1, steps: 142 loss: 3.696\n",
            "epochs:　1, steps: 143 loss: 3.850\n",
            "epochs:　1, steps: 144 loss: 3.630\n",
            "epochs:　1, steps: 145 loss: 3.695\n",
            "epochs:　1, steps: 146 loss: 3.778\n",
            "epochs:　1, steps: 147 loss: 3.670\n",
            "epochs:　1, steps: 148 loss: 3.689\n",
            "epochs:　1, steps: 149 loss: 3.608\n",
            "epochs:　1, steps: 150 loss: 3.518\n",
            "epochs:　1, steps: 151 loss: 3.593\n",
            "epochs:　1, steps: 152 loss: 3.748\n",
            "epochs:　1, steps: 153 loss: 3.517\n",
            "epochs:　1, steps: 154 loss: 3.567\n",
            "epochs:　1, steps: 155 loss: 3.641\n",
            "epochs:　1, steps: 156 loss: 3.584\n",
            "epochs:　1, steps: 157 loss: 3.506\n",
            "epochs:　1, steps: 158 loss: 3.677\n",
            "epochs:　1, steps: 159 loss: 3.748\n",
            "epochs:　1, steps: 160 loss: 3.448\n",
            "epochs:　1, steps: 161 loss: 3.609\n",
            "epochs:　1, steps: 162 loss: 3.547\n",
            "epochs:　1, steps: 163 loss: 3.677\n",
            "epochs:　1, steps: 164 loss: 3.608\n",
            "epochs:　1, steps: 165 loss: 3.492\n",
            "epochs:　1, steps: 166 loss: 3.644\n",
            "epochs:　1, steps: 167 loss: 3.621\n",
            "epochs:　1, steps: 168 loss: 3.627\n",
            "epochs:　1, steps: 169 loss: 3.641\n",
            "epochs:　1, steps: 170 loss: 3.495\n",
            "epochs:　1, steps: 171 loss: 3.537\n",
            "epochs:　1, steps: 172 loss: 3.324\n",
            "epochs:　1, steps: 173 loss: 3.612\n",
            "epochs:　1, steps: 174 loss: 3.586\n",
            "epochs:　1, steps: 175 loss: 3.571\n",
            "epochs:　1, steps: 176 loss: 3.560\n",
            "epochs:　1, steps: 177 loss: 3.444\n",
            "epochs:　1, steps: 178 loss: 3.525\n",
            "epochs:　1, steps: 179 loss: 3.660\n",
            "epochs:　1, steps: 180 loss: 3.604\n",
            "epochs:　1, steps: 181 loss: 3.402\n",
            "epochs:　1, steps: 182 loss: 3.662\n",
            "epochs:　1, steps: 183 loss: 3.563\n",
            "epochs:　1, steps: 184 loss: 3.534\n",
            "epochs:　1, steps: 185 loss: 3.511\n",
            "epochs:　1, steps: 186 loss: 3.481\n",
            "epochs:　1, steps: 187 loss: 3.455\n",
            "epochs:　1, steps: 188 loss: 3.332\n",
            "epochs:　1, steps: 189 loss: 3.531\n",
            "epochs:　1, steps: 190 loss: 3.496\n",
            "epochs:　1, steps: 191 loss: 3.530\n",
            "epochs:　1, steps: 192 loss: 3.483\n",
            "epochs:　1, steps: 193 loss: 3.396\n",
            "epochs:　1, steps: 194 loss: 3.597\n",
            "epochs:　1, steps: 195 loss: 3.335\n",
            "epochs:　1, steps: 196 loss: 3.363\n",
            "Finished Training\n",
            "Accuracy of the network on the test images: 11 %\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "    myClass=FeatureVisualization('./dog.jpg',7, models=my_model)\n",
        "    myClass.save_feature_to_img(epoch=epoch)\n",
        "    running_loss = 0.0\n",
        "    my_model.train()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        inputs = Variable(inputs.cuda())\n",
        "        labels = Variable(labels.cuda())\n",
        "      \n",
        "        # forward + backward + optimize\n",
        "        outputs = my_model(inputs)\n",
        "\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print('epochs:　%d, steps: %d loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
        "        running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    my_model.eval()\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = my_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy of the network on the test images: %d %%' % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoY7Ywni46Sc"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(cv2im, resize_im=True):\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        cv2im = cv2.resize(cv2im, (224, 224))\n",
        "    im_as_arr = np.float32(cv2im)\n",
        "    im_as_arr = np.ascontiguousarray(im_as_arr[..., ::-1])\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HvZ4bOj4824"
      },
      "outputs": [],
      "source": [
        "class Pretrained_VGGNet():\n",
        "    def __init__(self,img_path):\n",
        "        self.img_path=img_path\n",
        "        # Load pretrained model\n",
        "        self.pretrained_model = my_model\n",
        "        self.pretrained_model = self.pretrained_model.to(device)\n",
        "        self.pretrained_model.eval()\n",
        "    def process_image(self):\n",
        "        img=cv2.imread(self.img_path)\n",
        "        img=preprocess_image(img)\n",
        "        return img\n",
        "\n",
        "    def get_single_feature(self):\n",
        "        # Get the feature map\n",
        "        features=self.get_feature()\n",
        "        feature=features[:,0,:,:]\n",
        "        feature=feature.view(feature.shape[1],feature.shape[2])\n",
        "\n",
        "        #print(features)\n",
        "        #print(feature.shape)\n",
        "        return feature\n",
        "\n",
        "    def plot_probability(self,outputs):\n",
        "\n",
        "        outputs = outputs.data.cpu().numpy()\n",
        "        outputs = np.ndarray.tolist(outputs)\n",
        "\n",
        "        x = range(0, 100)        \n",
        "        plt.bar(x, outputs[0])\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Probability\")\n",
        "        plt.title(\"Image classifier\")\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self):\n",
        "        input=self.process_image()\n",
        "        input = input.to(device).cuda()\n",
        "        outputs = self.pretrained_model(input)\n",
        "\n",
        "        s = torch.nn.Softmax(dim=1)\n",
        "        result = s(outputs)\n",
        "        # self.plot_probability(result)\n",
        "\n",
        "        prob, predicted = result.sort(1,descending=True)\n",
        "        prob = prob.data.cpu().numpy()\n",
        "\n",
        "        predicted = predicted.data.cpu().numpy()\n",
        "        \n",
        "        print(\"Probability TOP-3:\\n\")\n",
        "        print(\"\")\n",
        "        for i in range(3):\n",
        "            \n",
        "            print(\"TOP_\"+str(i+1))\n",
        "            print(\"Probability:{}\".format(prob[0][i]))\n",
        "            print(\"Predicted:{}\\n\".format(c[int(predicted[0][i])]))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prkUerFp4-4W",
        "outputId": "ccb2119e-c959-4180-a5a2-b9ece27b5e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=1024, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "Probability TOP-3:\n",
            "\n",
            "\n",
            "TOP_1\n",
            "Probability:0.22863107919692993\n",
            "Predicted: 'sea'\n",
            "\n",
            "TOP_2\n",
            "Probability:0.15905150771141052\n",
            "Predicted: 'plain'\n",
            "\n",
            "TOP_3\n",
            "Probability:0.11801552772521973\n",
            "Predicted: 'mountain'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "  # get class\n",
        "  c = {}\n",
        "  with open(\"./CIFAR100_clsidx_to_labels.txt\") as f:\n",
        "    for line in f:\n",
        "      (key, val) = line.split(\":\")\n",
        "      c[int(key)] = val.split(\",\")[0]\n",
        "  # Define image path\n",
        "  myClass=Pretrained_VGGNet('./dog.jpg')\n",
        "  print(myClass.pretrained_model)\n",
        "  myClass.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnUOceel79gJ"
      },
      "source": [
        "##**Example 14~18**##\n",
        "##**Use the VGG-16 model train the CIFAR-100 dataset from scratch**##"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be0831dfa2c4161a344cca16231dc89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d6cab8050fb48be8393eaecf1ab9a29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e6f906da1784f8990586f2ce6974a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b50335e2b5949c5a054726675b82ce0",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b202d5386a6346a2adb9d40d83528148",
            "value": 553433881
          }
        },
        "25015bf4c85e4678ba47d0e29efdee0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d98e47f9f994dd58803803e22aff44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33cd3b4683824246bc0ff65dbc28fb44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914641222f7a4db5864a9d93aab1a5bf",
            "placeholder": "​",
            "style": "IPY_MODEL_2d98e47f9f994dd58803803e22aff44f",
            "value": "100%"
          }
        },
        "55de07f9573b4c55ac4dc719276b5b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "568ac00b2e944f249589a74212775845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be0831dfa2c4161a344cca16231dc89",
            "placeholder": "​",
            "style": "IPY_MODEL_ec564ffa02114d539314cdc1da407fa7",
            "value": " 528M/528M [00:04&lt;00:00, 97.3MB/s]"
          }
        },
        "59727d50ce744030956e17429411edba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8265db1cf159482f9718b78fe2a833e9",
              "IPY_MODEL_1e6f906da1784f8990586f2ce6974a70",
              "IPY_MODEL_568ac00b2e944f249589a74212775845"
            ],
            "layout": "IPY_MODEL_55de07f9573b4c55ac4dc719276b5b9b"
          }
        },
        "7305c53fb858407a8de9c3949e01d6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79bfb01f6c874aae861b54bfc6300d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b50335e2b5949c5a054726675b82ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8265db1cf159482f9718b78fe2a833e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c670505f2f68407ea432508c257161c1",
            "placeholder": "​",
            "style": "IPY_MODEL_79bfb01f6c874aae861b54bfc6300d28",
            "value": "100%"
          }
        },
        "914641222f7a4db5864a9d93aab1a5bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dabfb206db74bd3a18fea243160e06f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b202d5386a6346a2adb9d40d83528148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2097300b0604e5a8947a82dc5a53c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dabfb206db74bd3a18fea243160e06f",
            "placeholder": "​",
            "style": "IPY_MODEL_25015bf4c85e4678ba47d0e29efdee0f",
            "value": " 528M/528M [00:05&lt;00:00, 83.8MB/s]"
          }
        },
        "c620a4743ab746eaba119c5e0f88baeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33cd3b4683824246bc0ff65dbc28fb44",
              "IPY_MODEL_de1f61a36dd94a89a0310b757388a508",
              "IPY_MODEL_b2097300b0604e5a8947a82dc5a53c47"
            ],
            "layout": "IPY_MODEL_1d6cab8050fb48be8393eaecf1ab9a29"
          }
        },
        "c670505f2f68407ea432508c257161c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1f61a36dd94a89a0310b757388a508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7305c53fb858407a8de9c3949e01d6f3",
            "max": 553507836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eaa851f77b2f47b796d8cb25191e8945",
            "value": 553507836
          }
        },
        "eaa851f77b2f47b796d8cb25191e8945": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec564ffa02114d539314cdc1da407fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
